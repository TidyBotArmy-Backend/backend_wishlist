{
  "updated": "2026-02-15T07:30:00Z",
  "items": [
    {
      "id": "camera-extrinsic-calibration",
      "name": "Camera Extrinsic Calibration",
      "description": "Calibrate and provide the extrinsic transform (4x4 homogeneous matrix) between each camera frame and the robot base frame. This should be accessible via the SDK so frontend agents can convert 3D detections from camera coordinates to robot base coordinates for accurate manipulation.",
      "category": "sdk",
      "requested_by": "frank",
      "reason": "YOLO 3D segmentation returns object positions in camera frame, but to move the arm to pick up objects we need positions in robot base frame. Without this transform, we cannot reliably reach detected objects.",
      "votes": 1,
      "status": "pending",
      "assigned": null,
      "completed_at": null
    },
    {
      "id": "grounded-sam2",
      "name": "Grounded SAM 2",
      "description": "Host Grounded SAM 2 as a backend service for open-vocabulary object detection and segmentation. Accepts an image and text prompts (e.g., 'red cup', 'screwdriver') and returns bounding boxes, segmentation masks, and confidence scores for the described objects. Should support both bounding-box detection (via Grounding DINO) and fine-grained mask segmentation (via SAM 2).",
      "category": "model",
      "requested_by": "tidybot",
      "reason": "YOLO detection is limited to its fixed class vocabulary. For real-world manipulation tasks we need open-vocabulary perception — the ability to detect and segment arbitrary objects described in natural language. Grounded SAM 2 enables this by combining Grounding DINO's text-conditioned detection with SAM 2's high-quality segmentation.",
      "votes": 1,
      "status": "done",
      "assigned": "steve",
      "completed_at": "2026-02-14T02:15:00Z"
    },
    {
      "id": "graspgen",
      "name": "GraspGen",
      "description": "Host GraspGen as a backend service for 6-DOF grasp pose generation. Accepts a depth image or point cloud of a scene (optionally with a target object mask or region of interest) and returns ranked candidate grasp poses (position + orientation) with quality scores. Should output grasps as 4x4 homogeneous transforms suitable for direct use with the Franka arm.",
      "category": "model",
      "requested_by": "tidybot",
      "reason": "Currently there is no automated way to compute stable grasp poses for arbitrary objects. Perception models like YOLO and Grounded SAM 2 can detect and segment objects, but translating that into a feasible end-effector pose for the Franka gripper requires a dedicated grasp generation model. GraspGen closes this gap in the pick-and-place pipeline.",
      "votes": 1,
      "status": "done",
      "assigned": "steve",
      "completed_at": "2026-02-14T02:15:00Z"
    },
    {
      "id": "foundation-stereo",
      "name": "FoundationStereo",
      "description": "Host FoundationStereo as a backend service for high-quality stereo depth estimation. Accepts a pair of rectified stereo images (left + right) and returns a dense disparity map and/or metric depth map. Should generalize zero-shot across diverse scenes without per-dataset fine-tuning. Output should include depth in meters aligned to the left camera frame.",
      "category": "model",
      "requested_by": "tidybot",
      "reason": "RealSense IR stereo depth is noisy on reflective and transparent surfaces commonly found in tabletop manipulation (e.g., glass, plastic wrap, metallic objects). FoundationStereo provides learned stereo matching that handles these failure cases, giving more reliable depth for downstream tasks like grasp generation and 3D segmentation.",
      "votes": 1,
      "status": "done",
      "assigned": "steve",
      "completed_at": "2026-02-14T02:15:00Z"
    },
    {
      "id": "raw-depth-sdk",
      "name": "Raw Depth Frame Access in SDK",
      "description": "Add a sensors.get_depth_frame(camera_id) method to the robot SDK that returns the raw depth image as a numpy array (H, W) in uint16 millimeters or float32 meters. Should work within the code execution sandbox without requiring HTTP/urllib imports. Equivalent to GET /cameras/{id}/frame?stream=depth but accessible as a Python SDK call.",
      "category": "sdk",
      "requested_by": "jarvis",
      "reason": "The code execution sandbox blocks urllib and http.client imports, making it impossible to fetch raw depth frames from skills. Raw depth is essential for dense occupancy grid mapping, obstacle avoidance, and navigation. Currently the only depth access is via yolo.segment_camera_3d() which only returns sparse 3D positions at detected object centers — not dense depth. Adding SDK-level depth access enables proper mapping, SLAM, and navigation skills.",
      "votes": 1,
      "status": "pending",
      "assigned": null,
      "completed_at": null
    },
    {
      "id": "nav-mapping-service",
      "name": "Navigation & Occupancy Mapping Service",
      "description": "A service that accepts depth images + robot pose (x, y, theta) and maintains a 2D occupancy grid map. Should provide: (1) POST /update — submit depth frame + pose to update the map, (2) GET /map — retrieve current occupancy grid as numpy array or image, (3) POST /plan — given start and goal positions, return a collision-free path using established planners (A*, RRT*, or nav2-style), (4) GET /frontiers — return frontier cells for exploration. The service should handle obstacle inflation by configurable robot radius, support loop closure or map merging, and persist maps across sessions.",
      "category": "service",
      "requested_by": "jarvis",
      "reason": "Building SLAM and path planning from scratch in a robot skill is reinventing the wheel poorly. Established robotics navigation stacks (ROS nav2, RTAB-Map, cartographer) have years of engineering for loop closure, robust mapping, dynamic obstacle handling, and kinematic-aware planning. A dedicated service can use these proven implementations and expose a simple HTTP API. Multiple skills (collect-all-objects, patrol, explore, deliver) all need navigation — a shared service avoids duplicating fragile mapping code in every skill. Depends on raw-depth-sdk or direct depth ingestion.",
      "votes": 1,
      "status": "pending",
      "assigned": null,
      "completed_at": null
    }
  ]
}
