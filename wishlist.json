{
  "updated": "2026-02-14T02:15:00Z",
  "items": [
    {
      "id": "camera-extrinsic-calibration",
      "name": "Camera Extrinsic Calibration",
      "description": "Calibrate and provide the extrinsic transform (4x4 homogeneous matrix) between each camera frame and the robot base frame. This should be accessible via the SDK so frontend agents can convert 3D detections from camera coordinates to robot base coordinates for accurate manipulation.",
      "category": "sdk",
      "requested_by": "frank",
      "reason": "YOLO 3D segmentation returns object positions in camera frame, but to move the arm to pick up objects we need positions in robot base frame. Without this transform, we cannot reliably reach detected objects.",
      "votes": 1,
      "status": "pending",
      "assigned": null,
      "completed_at": null
    },
    {
      "id": "grounded-sam2",
      "name": "Grounded SAM 2",
      "description": "Host Grounded SAM 2 as a backend service for open-vocabulary object detection and segmentation. Accepts an image and text prompts (e.g., 'red cup', 'screwdriver') and returns bounding boxes, segmentation masks, and confidence scores for the described objects. Should support both bounding-box detection (via Grounding DINO) and fine-grained mask segmentation (via SAM 2).",
      "category": "model",
      "requested_by": "tidybot",
      "reason": "YOLO detection is limited to its fixed class vocabulary. For real-world manipulation tasks we need open-vocabulary perception \u2014 the ability to detect and segment arbitrary objects described in natural language. Grounded SAM 2 enables this by combining Grounding DINO's text-conditioned detection with SAM 2's high-quality segmentation.",
      "votes": 1,
      "status": "done",
      "assigned": "steve",
      "completed_at": "2026-02-14T02:15:00Z"
    },
    {
      "id": "graspgen",
      "name": "GraspGen",
      "description": "Host GraspGen as a backend service for 6-DOF grasp pose generation. Accepts a depth image or point cloud of a scene (optionally with a target object mask or region of interest) and returns ranked candidate grasp poses (position + orientation) with quality scores. Should output grasps as 4x4 homogeneous transforms suitable for direct use with the Franka arm.",
      "category": "model",
      "requested_by": "tidybot",
      "reason": "Currently there is no automated way to compute stable grasp poses for arbitrary objects. Perception models like YOLO and Grounded SAM 2 can detect and segment objects, but translating that into a feasible end-effector pose for the Franka gripper requires a dedicated grasp generation model. GraspGen closes this gap in the pick-and-place pipeline.",
      "votes": 1,
      "status": "done",
      "assigned": "steve",
      "completed_at": "2026-02-14T02:15:00Z"
    },
    {
      "id": "foundation-stereo",
      "name": "FoundationStereo",
      "description": "Host FoundationStereo as a backend service for high-quality stereo depth estimation. Accepts a pair of rectified stereo images (left + right) and returns a dense disparity map and/or metric depth map. Should generalize zero-shot across diverse scenes without per-dataset fine-tuning. Output should include depth in meters aligned to the left camera frame.",
      "category": "model",
      "requested_by": "tidybot",
      "reason": "RealSense IR stereo depth is noisy on reflective and transparent surfaces commonly found in tabletop manipulation (e.g., glass, plastic wrap, metallic objects). FoundationStereo provides learned stereo matching that handles these failure cases, giving more reliable depth for downstream tasks like grasp generation and 3D segmentation.",
      "votes": 1,
      "status": "done",
      "assigned": "steve",
      "completed_at": "2026-02-14T02:15:00Z"
    }
  ]
}
