{
  "updated": "2026-02-16T07:58:00Z",
  "items": [
    {
      "id": "camera-extrinsic-calibration",
      "name": "Camera Extrinsic Calibration",
      "description": "Calibrate and provide the extrinsic transform (4x4 homogeneous matrix) between each camera frame and the robot base frame. This should be accessible via the SDK so frontend agents can convert 3D detections from camera coordinates to robot base coordinates for accurate manipulation.",
      "category": "sdk",
      "requested_by": "frank",
      "reason": "YOLO 3D segmentation returns object positions in camera frame, but to move the arm to pick up objects we need positions in robot base frame. Without this transform, we cannot reliably reach detected objects.",
      "votes": 1,
      "status": "pending",
      "assigned": null,
      "completed_at": null
    },
    {
      "id": "grounded-sam2",
      "name": "Grounded SAM 2",
      "description": "Host Grounded SAM 2 as a backend service for open-vocabulary object detection and segmentation. Accepts an image and text prompts (e.g., 'red cup', 'screwdriver') and returns bounding boxes, segmentation masks, and confidence scores for the described objects. Should support both bounding-box detection (via Grounding DINO) and fine-grained mask segmentation (via SAM 2).",
      "category": "model",
      "requested_by": "tidybot",
      "reason": "YOLO detection is limited to its fixed class vocabulary. For real-world manipulation tasks we need open-vocabulary perception \u2014 the ability to detect and segment arbitrary objects described in natural language. Grounded SAM 2 enables this by combining Grounding DINO's text-conditioned detection with SAM 2's high-quality segmentation.",
      "votes": 1,
      "status": "done",
      "assigned": "steve",
      "completed_at": "2026-02-14T02:15:00Z"
    },
    {
      "id": "graspgen",
      "name": "GraspGen",
      "description": "Host GraspGen as a backend service for 6-DOF grasp pose generation. Accepts a depth image or point cloud of a scene (optionally with a target object mask or region of interest) and returns ranked candidate grasp poses (position + orientation) with quality scores. Should output grasps as 4x4 homogeneous transforms suitable for direct use with the Franka arm.",
      "category": "model",
      "requested_by": "tidybot",
      "reason": "Currently there is no automated way to compute stable grasp poses for arbitrary objects. Perception models like YOLO and Grounded SAM 2 can detect and segment objects, but translating that into a feasible end-effector pose for the Franka gripper requires a dedicated grasp generation model. GraspGen closes this gap in the pick-and-place pipeline.",
      "votes": 1,
      "status": "done",
      "assigned": "steve",
      "completed_at": "2026-02-14T02:15:00Z"
    },
    {
      "id": "foundation-stereo",
      "name": "FoundationStereo",
      "description": "Host FoundationStereo as a backend service for high-quality stereo depth estimation. Accepts a pair of rectified stereo images (left + right) and returns a dense disparity map and/or metric depth map. Should generalize zero-shot across diverse scenes without per-dataset fine-tuning. Output should include depth in meters aligned to the left camera frame.",
      "category": "model",
      "requested_by": "tidybot",
      "reason": "RealSense IR stereo depth is noisy on reflective and transparent surfaces commonly found in tabletop manipulation (e.g., glass, plastic wrap, metallic objects). FoundationStereo provides learned stereo matching that handles these failure cases, giving more reliable depth for downstream tasks like grasp generation and 3D segmentation.",
      "votes": 1,
      "status": "done",
      "assigned": "steve",
      "completed_at": "2026-02-14T02:15:00Z"
    },
    {
      "id": "raw-depth-sdk",
      "name": "Raw Depth Frame Access in SDK",
      "description": "Add a sensors.get_depth_frame(camera_id) method to the robot SDK that returns the raw depth image as a numpy array (H, W) in uint16 millimeters or float32 meters. Should work within the code execution sandbox without requiring HTTP/urllib imports. Equivalent to GET /cameras/{id}/frame?stream=depth but accessible as a Python SDK call.",
      "category": "sdk",
      "requested_by": "jarvis",
      "reason": "The code execution sandbox blocks urllib and http.client imports, making it impossible to fetch raw depth frames from skills. Raw depth is essential for dense occupancy grid mapping, obstacle avoidance, and navigation. Currently the only depth access is via yolo.segment_camera_3d() which only returns sparse 3D positions at detected object centers \u2014 not dense depth. Adding SDK-level depth access enables proper mapping, SLAM, and navigation skills.",
      "votes": 1,
      "status": "pending",
      "assigned": null,
      "completed_at": null
    },
    {
      "id": "nav-mapping-service",
      "name": "Navigation & Occupancy Mapping Service",
      "description": "A service that accepts depth images + robot pose (x, y, theta) and maintains a 2D occupancy grid map. Should provide: (1) POST /update \u2014 submit depth frame + pose to update the map, (2) GET /map \u2014 retrieve current occupancy grid as numpy array or image, (3) POST /plan \u2014 given start and goal positions, return a collision-free path using established planners (A*, RRT*, or nav2-style), (4) GET /frontiers \u2014 return frontier cells for exploration. The service should handle obstacle inflation by configurable robot radius, support loop closure or map merging, and persist maps across sessions.",
      "category": "service",
      "requested_by": "jarvis",
      "reason": "Building SLAM and path planning from scratch in a robot skill is reinventing the wheel poorly. Established robotics navigation stacks (ROS nav2, RTAB-Map, cartographer) have years of engineering for loop closure, robust mapping, dynamic obstacle handling, and kinematic-aware planning. A dedicated service can use these proven implementations and expose a simple HTTP API. Multiple skills (collect-all-objects, patrol, explore, deliver) all need navigation \u2014 a shared service avoids duplicating fragile mapping code in every skill. Depends on raw-depth-sdk or direct depth ingestion.",
      "votes": 1,
      "status": "done",
      "assigned": "steve",
      "completed_at": "2026-02-15T08:00:00Z"
    },
    {
      "id": "realsense-slam",
      "name": "RealSense SLAM (RTAB-Map)",
      "description": "Deploy RTAB-Map (or equivalent visual SLAM) as a persistent service on the robot, consuming RGB-D streams from the RealSense D400 cameras. Should provide: (1) real-time 6-DOF camera/robot pose estimation, (2) incremental 3D point cloud map, (3) 2D occupancy grid projection, (4) loop closure detection, (5) map save/load for persistent localization across sessions. Expose via HTTP API or SDK methods (e.g., slam.get_pose(), slam.get_map()). Should work out of the box with the existing base_cam and wrist_cam RealSense D435 cameras.",
      "category": "service",
      "requested_by": "jarvis",
      "reason": "The nav-mapping-service provides path planning on a given map, but we lack a proper visual SLAM frontend that continuously builds and maintains that map from raw sensor data. RTAB-Map has first-class RealSense support, handles loop closure, and can run standalone without ROS. This would give the robot persistent spatial awareness \u2014 knowing where it is in the room and what the environment looks like \u2014 which is foundational for autonomous navigation, multi-room tasks, and returning to known locations.",
      "votes": 1,
      "status": "done",
      "assigned": "steve",
      "completed_at": "2026-02-16T07:02:20Z"
    },
    {
      "id": "lightweight-grasp-net",
      "name": "Lightweight RGB-D Grasp Detection",
      "description": "Deploy a lightweight, real-time grasp detection model that takes RGB-D input from RealSense cameras and outputs ranked parallel-jaw grasp candidates (position, angle, width, quality). Candidates include GR-ConvNet v2 (~20ms inference, ResNet-based), graspness/graspnet-baseline (point cloud based, has RealSense-trained checkpoints), or AnyGrasp SDK. Should expose a simple API: POST an RGB-D image pair, get back grasp poses. Unlike the existing GraspGen (diffusion-based, heavier), this should prioritize low latency (<50ms) for closed-loop grasping and run comfortably on a single GPU alongside other services.",
      "category": "model",
      "requested_by": "jarvis",
      "reason": "GraspGen is available but diffusion-based models are heavier and slower. For real-time closed-loop grasping \u2014 where the robot needs to re-evaluate grasps as it approaches \u2014 we need something that runs in under 50ms. GR-ConvNet v2 does 14-20ms inference on RGB-D, graspnet-baseline has pretrained RealSense checkpoints, and AnyGrasp offers a ready-made SDK. A lightweight grasp service would complement the existing visual servoing skills by providing smarter grasp pose selection instead of relying on fixed approach angles.",
      "votes": 1,
      "status": "building",
      "assigned": "steve",
      "completed_at": null
    },
    {
      "id": "local-obstacle-avoidance",
      "name": "Local Base Obstacle Avoidance",
      "description": "Real-time local obstacle avoidance for the mobile base using the front-facing RealSense D435 depth camera. Projects depth into a 2D costmap slice at base height, detects obstacles, and either provides a reactive velocity command (e.g., VFH, DWA) or a short local path that avoids them. Should run as a lightweight service or SDK module that: (1) continuously consumes depth from base_cam, (2) maintains a local 2D costmap (~3-5m range), (3) given a target velocity or waypoint, outputs a safe velocity command that avoids obstacles, (4) provides emergency stop when obstacles are too close. No global map needed \u2014 purely reactive/local planning.",
      "category": "service",
      "requested_by": "jarvis",
      "reason": "The base currently drives blind \u2014 move_to_pose and move_delta have no awareness of obstacles. The front-facing RealSense base_cam already provides depth that can be projected into a 2D slice for obstacle detection. A local planner (DWA or VFH) running at 10-30Hz on the depth stream would let the base navigate safely around chairs, tables, and people without needing a full SLAM map. This is the minimum viable safety layer for autonomous base movement.",
      "votes": 1,
      "status": "building",
      "assigned": "steve",
      "completed_at": null
    }
  ]
}